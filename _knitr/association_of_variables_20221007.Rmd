---
title: "Common Statistical Tests in R"
author: "Martin Chan"
date: "October 7, 2022"
output:                    # DO NOT CHANGE
  prettydoc::html_pretty:  # DO NOT CHANGE
    theme: cayman          # DO NOT CHANGE
    highlight: github      # DO NOT CHANGE
---

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
library(tidyverse)
# prettyjekyll::FormatPost("_knitr/association_of_variables_20221007.Rmd")
```

# Introduction

This post will focus on common statistical tests in R that you can perform to understand and validate the relationship between two variables. 

Very statistics 101, you may be thinking. So why? This post is edited from my own notes from learning statistics and R. The primary goal of the post is to be practical enough for myself to refer to from time to time, as I do for all my other tutorials/posts in this blog. The secondary goal is that hopefully, as a side effect, that this post will benefit others who are learning statistics and R too. 

To illustrate the R code, I will also be using a sample dataset `pq_data` from the package [**vivainsights**](https://microsoft.github.io/vivainsights/), which is a cross-sectional time-series dataset measuring the collaboration behaviour of simulated employees in an organization. Each row represents an employee on a certain week, with columns measuring behaviours such as total weekly time spent in email, meetings, chats, and so on. 

A note about the structure of this post: in the real world, one should always as best practice visually check the data distribution and run tests for assumptions like normality and homoscedasticity prior to performing any tests. This best practice isn't really observed in this post for the sake of covering the widest range of scenarios. Hence, please be forgiving when you see that the narrative runs 'head first' into a test without examining the data - and avoid this in real life!

# Loading the dataset

The package **vivainsights** is available on CRAN, so you can install this with `install.packages("vivainsights")`.

You can load the dataset in R by calling `pq_data` after loading the **vivainsights** package. Here is a preview of the first ten columns of the dataset using `dplyr::glimpse()`:

```{r message=FALSE, warning=FALSE}
library(vivainsights)

glimpse(pq_data[, 1:10])
```

This tutorial also uses functions from **tidyverse**, so ensure that you run `library(tidyverse)` to reproduce the example outputs. 

# Understanding the relation between two variables

One of the most basic tasks in statistics and data science is to understand the relation between two variables. Sometimes the motivation is understand whether the relationship is causal, but this is not always the case (for instance, one may simply wish to test for multicollinearity when selecting predictors for a model).

In our dataset, we have two metrics of interest:
  -  `Multitasking_hours` measures the total number of hours the person spent sending emails or instant messages during a meeting or a Teams call.
  - `After_hours_collaboration_hours` measures the number of hours a person has spent in collaboration (meetings, emails, IMs, and calls) outside of working hours.[^1]

[^1]: See <https://learn.microsoft.com/en-us/viva/insights/use/metric-definitions> for definitions.

Imagine then we have two questions to address: 

1. We suspect that multitasking hours is correlated with after-hours working, as the former could be a symptom of excessive workload or sub-optimal time management which can explain after-hours working. What can we do to understand the relationship between the two? 

1. Do _managers_ multi-task more than _senior individual contributors (IC)_? 

To answer these questions, there are a couple of common methods at our disposal. Here is a (non-exhaustive) list:

  - Comparison tests
  - Correlation tests
  - Regression tests
  - Effect Size
  - Statistical power
  - Sample variability

It is worth noting that the first question postulates a relation between two **continuous** variables (multitasking hours, afterhours collaboration), whereas the second question a relation between a **categorical** variable (manager/ senior IC) and a **continuous** variable (multitasking hours). The types of the variables in question help determine which tests are appropriate.

The categorical variable that provides us information on whether an employee is a manager or a senior IC in `pq_data` is stored in `LevelDesignation`. We can use `vivainsights::hrvar_count()` to explore this variable: 

```{r message=FALSE, warning=FALSE}
hrvar_count(pq_data, hrvar = "LevelDesignation")
```

## Comparison tests

Two of the most common comparison tests would be the **t-test** and **Analysis of Variance (ANOVA)**. 

A t-test can be paired or unpaired, where the former is used for comparing the means of two groups in the same population, and the latter for independent samples from two populations or groups. An unpaired (two-sample) t-test is therefore appropriate for the scenario in question two, as managers and ICs are two different populations. 

Since we are interested in the difference between managers and senior ICs, we will first need to create a factor variable from the data that has only two levels. In the below code, we will first filter out any values of `LevelDesignation` that are not `"Manager"` and `"Senior IC"`, and create a new factor column as `ManagerIndicator`:

```{r message=FALSE, warning=FALSE}
pq_data_grouped <-
  pq_data %>%
  filter(LevelDesignation %in% c("Manager", "Senior IC")) %>%
  mutate(
    ManagerIndicator =
      factor(LevelDesignation,
      levels = c("Manager", "Senior IC"))
  )
```

Recall also that our dataset `pq_data` is a cross-sectional time-series dataset, which means that for every individual identified by `PersonId`, there will be multiple rows corresponding to different dates. To simplify the dataset so that we are looking at person averages, we can group the dataset by `PersonId` and calculate the mean of `Multitasking_hours`. After this manipulation, `Multitasking_hours` would represent the mean multitasking hours _per person_, as opposed to _per person per week_. Let us do this by extending the pipe:


```{r message=FALSE, warning=FALSE}
pq_data_grouped <-
  pq_data %>%
  filter(LevelDesignation %in% c("Manager", "Senior IC")) %>%
  mutate(
    ManagerIndicator =
      factor(LevelDesignation,
      levels = c("Manager", "Senior IC"))
  ) %>%
  group_by(PersonId, ManagerIndicator) %>%
  summarise(Multitasking_hours = mean(Multitasking_hours), .groups = "drop")
  
glimpse(pq_data_grouped)
```

Now our data is in the right format. Let us assume that the data satisfies all the assumptions of the t-test, and see what happens when we run it with the base `t.test()` function:

```{r message=FALSE, warning=FALSE}
t.test(
  Multitasking_hours ~ ManagerIndicator,
  data = pq_data_grouped,
  paired = FALSE
)
```

In the function, the predictor and outcome variables are supplied using a tilde (`~`) format common in R, and we have specified `paired = FALSE` to use an unpaired t-test. As for the output, 

- `t` represents the t-statistic.
- `df` represents the degree of freedom. 
- `p-value` is - well - the p-value. The value here shows to be significant, as it is smaller than the significance level at 0.05. 
- the test allows us to reject the null hypothesis that the means of multitasking hours between managers and ICs are the same.

Note that the t-test used here is the **Welch's t-test**, which is an adaptation of the classic **Student's t-test** in that it compares the variance of the two groups, i.e. handling heteroscedasticity.

### Testing for normality

But hang on! 

There are several assumptions behind the unpaired classic t-test, notably: 

1. independence (sample is independent)
1. normality (data is normally distributed)
1. homoscedasticity (data across samples have equal variance)

We can at least be sure of (1), as we know that senior ICs and Managers are separate populations. However, (2) and (3) are assumptions that we have to validate specifically. To test whether our data is normally distributed, we can use the **Shapiro-Wilk test of normality**, with the function `shapiro.test()`:

```{r message=FALSE, warning=FALSE}
pq_data_grouped %>%
  group_by(ManagerIndicator) %>%
  summarise(
    p = shapiro.test(Multitasking_hours)$p.value,
    statistic = shapiro.test(Multitasking_hours)$statistic
  )
```

As both p-values show up as less than 0.05, the test implies that we should reject the null hypothesis that the data are normally distributed (i.e. not normally distributed). To confirm, you can also perform a visual check for normality using a histogram or a Q-Q plot. 

```{r message=FALSE, warning=FALSE}
# Multitasking hours - IC
mth_ic <-
  pq_data_grouped %>%
  filter(ManagerIndicator == "Senior IC") %>%
  pull(Multitasking_hours) 

qqnorm(mth_ic, pch = 1, frame = FALSE)
qqline(mth_ic, col = "steelblue", lwd = 2)

# Multitasking hours - Manager
mth_man <-
  pq_data_grouped %>%
  filter(ManagerIndicator == "Manager") %>%
  pull(Multitasking_hours) 

qqnorm(mth_man, pch = 1, frame = FALSE)
qqline(mth_man, col = "steelblue", lwd = 2)
```

As our data isn't as normal as we would wish it to be - this makes the unpaired t-test less appropriate or conclusive, and we can consider other alternatives such as the **non-parametric two-samples Wilcoxon Rank-Sum test**. This is covered further down below. 

### Testing for equality of variance (homoscedasticity)

Asides from normality, another assumption of the t-test that we didn't properly test for prior to running `t.test()` is to check for equality of variance across the two groups (homoscedasticity). Thankfully, this was not something we had to worry about as we used the Welch's t-test; recall that the classic Student's t-test assumes equality between the two variances, but the Welch's t-test takes the difference into account. 

Regardless, here is an example on how you can test for homoscedasticity in R, using `var.test()`: 

```{r message=FALSE, warning=FALSE}
# F test to compare two variances
var.test(
  Multitasking_hours ~ ManagerIndicator,
  data = pq_data_grouped
  )
```

The arguments above are provided in a similar format to `t.test()`. It also appears that homoscedasticity does not hold: since the p-value is less than 0.05, we should reject the null hypothesis that variances between the manager and IC dataset are equal. 

Homoscedasticity can also be examined visually, via a boxplot or a dotplot (using `graphics::dotchart()` - suitable for small datasets). The code to do so would be as follows. For this example, visual examination is a bit more challenging as the senior IC and Manager groups have starkly different levels of multi-tasking hours. 

```{r message=FALSE, warning=FALSE}
dotchart(
  x = pq_data_grouped$Multitasking_hours,
  groups = pq_data_grouped$ManagerIndicator
)

boxplot(
  Multitasking_hours ~ ManagerIndicator,
  data = pq_data_grouped
)
```

## Non-parametric tests

Previously, we could not safely rely on the unpaired two-sample t-test because the data does not satisfy the normality condition. As an alternative, we can use the **Wilcoxon Rank-Sum test** (aka Mann Whitney U Test). The difference between the Wilcoxon Rank-Sum test and the unpaired t-test is that the former tests whether two populations have the same shape via comparing medians, whereas the latter parametric test compares means between two independent groups. 

The Wilcoxon test is described as a **non-parametric test**, which in statistics typically means that there is no specification on a distribution, or the parameters of a distribution; in this case, the test does not assume a normal distribution. 

This is run using `wilcox.test()`

```{r message=FALSE, warning=FALSE}
wilcox.test(
  Multitasking_hours ~ ManagerIndicator,
  data = pq_data_grouped,
  paired = FALSE
)
```

The p-value of the test is less than the significance level (alpha = 0.05), which allows us to conclude that Manager's median multitasking hours is significantly different from the IC's. 

Note that the Wilcoxon Rank-Sum test is different from the similarly named Wilcoxon Signed-Rank test, which is the equivalent alternative for the _paired_ t-test. To perform the Wilcoxon Signed-Rank test instead, you can simply specify the argument to be `paired = TRUE`. Similar to the decision of whether to use the paired or the unpaired t-test, you should ensure that the one-sample condition applies if you use the Wilcoxon Signed-Rank test.

So far, we have only been looking at tests which compare exactly two populations. If we are looking for a test that works with comparisons across three or more populations, we can consider the **Kruskal-Wallis test**. 

Let us create a new data frame that is grouped at the `PersonId` level, but filtering out fewer values in `LevelDesignation`: 

```{r message=FALSE, warning=FALSE}
pq_data_grouped_2 <-
  pq_data %>%
  filter(LevelDesignation %in% c(
    "Support",
    "Senior IC",
    "Junior IC",
    "Manager",
    "Director"
  )) %>%
  mutate(ManagerIndicator = factor(LevelDesignation)) %>%
  group_by(PersonId, ManagerIndicator) %>%
  summarise(Multitasking_hours = mean(Multitasking_hours), .groups = "drop")
  
glimpse(pq_data_grouped_2)
```
We can then run the Kruskal-Wallis test:

```{r message=FALSE, warning=FALSE}
kruskal.test(
  Multitasking_hours ~ ManagerIndicator,
  data = pq_data_grouped_2
)
```

Based on the Kruskal-Wallis test, we reject the null hypothesis and we conclude that at least one value in `LevelDesignation` is different in terms of their weekly hours spent multitasking. The most obvious downside to this method is that it does not tell us which groups are different from which, so this may need to be followed up with multiple pairwise-comparison tests.


## ANOVA - comparing across multiple groups

What if we want to run the t-test across multiple (more than two) groups?

Analysis of Variance (ANOVA) is an alternative method that generalises the t-test beyond two groups, so it is used to compare three or more groups.

There are several versions of ANOVA. The simple version is the _one-way ANOVA_, but there is also _two-way ANOVA_ which is used to estimate how the mean of a quantitative variable changes according to the levels of two categorical variables (e.g. rain/no-rain and weekend/weekday to predict ice cream sales). In this example we will focus on one-way ANOVA. 

There are three assumptions in ANOVA:
  - The data are independent.
  - The responses for each factor level have a normal population distribution.
  - These distributions have the same variance.

These assumptions are the same as those required for the classic t-test above, and you can run the same tests for variance and normality prior to ANOVA. 

ANOVA calculates the ratio of the between-group variance and the within-group variance, and then compares this with a threshold from the Fisher distribution (typically based on a significance level).

```{r message=FALSE, warning=FALSE}
res_aov <-
  aov(
    Multitasking_hours ~ ManagerIndicator,
    data = pq_data_grouped_2
  )

summary(res_aov)
```

- `Df`: degrees of freedom for...
  - the outcome variable, i.e. the number of levels in the variable minus 1
  - the residuals, i.e. the total number of observations minus one and minus the number of levels in the outcome variables
  
- `Sum Sq`: sum of squares, i.e. the total variation between the group means and the overall mean

- `Mean Sq`: mean of the sum of squares, calculated by dividing the sum of squares by the degrees of freedom for each parameter

- `F value`: test statistic from the F test. This is the mean square of each independent variable divided by the mean square of the residuals. The larger the F value, the more likely it is that the variation caused by the outcome variable is real and not due to chance.

- `Pr(>F)`: p-value of the F-statistic. This shows how likely it is that the F-value calculated from the test would have occurred if the null hypothesis of no difference among group means were true.


Given that the p-value is smaller than 0.05, we reject the null hypothesis, so we reject the hypothesis that all means are equal. Therefore, we can conclude that the at least one value in `LevelDesignation` is different in terms of their weekly hours spent multitasking.

[Antoine Soetewey's blog](https://statsandr.com/blog/anova-in-r/) recommends the use of the **report** package, which can help you make sense of the results more easily:

```{r message=FALSE, warning=FALSE}
library(report)

report(res_aov)
```

The same drawback that applies to the Kruskall-Wallis test also applies to ANOVA, in that doesn't actually tell you which exact group is different from which; it only tells you whether any group differs significantly from the group mean. 

A pairwise t-test is likely required to provide more information, and it is recommended that you review the [p-value adjustment methods](https://rdrr.io/r/stats/p.adjust.html): 

```{r message=FALSE, warning=FALSE}
pairwise.t.test(
  x = pq_data_grouped_2$Multitasking_hours,
  g = pq_data_grouped_2$ManagerIndicator,
  paired = FALSE,
  p.adjust.method = "bonferroni"
)
```


## Should I use a t-test or ANOVA for comparing exactly two groups?

So far, the following tests we performed have yielded similar results:

For comparing Senior ICs and Managers:
- unpaired two-sample t-test (assumes normality)
- Wilcoxon Rank-Sum test (non-parametric)

For comparing across other values in `LevelDesignation`:
- Kruskal-Wallis test (non-parametric)
- ANOVA (assumes normality)

One common question is - given that normality is observed in each group, what difference does it make whether I use the t-test or ANOVA if I am comparing exactly two groups? 

The textbook advice is that whenever one is comparing exactly two groups one should use the t-test, and ANOVA whenever there are more than two groups being compared. It is in fact not _wrong_ per say to use ANOVA for comparing two groups, as it will be equivalent to a classic (Student's) t-test with equal variances.[^2] A reason why one may prefer the Welch's t-test over ANOVA is that the Welch's t-test takes into account heteroscedasticity, whereas ANOVA (as well as Kruskall-Wallis) would become unstable and produce Type I errors, such as: 

- conservative estimates for large sample sizes
- inflated estimates for small sample size[^3]

[^2]: See [this discussion](https://stats.stackexchange.com/questions/236877/is-it-wrong-to-use-anova-instead-of-a-t-test-for-comparing-two-means) and [this](https://stats.stackexchange.com/questions/409503/anova-vs-t-test-for-two-groups).

[^3]: https://www.statisticshowto.com/welchs-anova/


## ANOVA, t-tests, and linear regression

The common assumptions shared may have gave it away, but the t-test, ANOVA, and linear regression are actually related, in that one is a special case of another. 

The t-test is considered as a special case of ANOVA, since the classic Student's t-test is the same as ANOVA in comparing two groups when variances are equal. When the t-test statistic is squared, you get the corresponding F in the ANOVA.

An ANOVA model is the same as a reression with a dummy variable. In fact, the `aov()` function in R is a wrapper around the linear regression function `lm()`.



- The t-test with two groups assumes that each group is normally distributed with the same variance.
- The t-test is the same as a regression with a dummy variable, which allows the mean of each group to differ but not the variance. 
- The `aov()` function in R is a wrapper around the linear regression function `lm()`.


The t-test is a special case of ANOVA. ANOVA is a special case of regression. All of these procedures are subsumed under the General Linear Model and share the same assumptions.



## Correlations

Correlation tests are used in statistics to measure how strong a relationship is between two variables, without hypothesising any causal effect between the variables. There are several types of correlation coefficients (e.g. Pearson's _r_, Kendall's _tau_, Spearman's _rho_), but the most commonly used is the Pearson's correlation coefficient.

Correlation tests are a form of **non-parametric test**, which don't make as many assumptions about the data and are useful when one or more of the common statistical assumptions are violated. However, the inferences they make aren't as strong as with parametric tests.

Correlation is a way to test if two variables have any kind of relationship, whereas p-value tells us if the result of an experiment is statistically significant. 


The statistical significance of a correlation can be evaluated with the t-statistic. This can be yielded with `cor.test()` in R:

```r
cor.test()
```

This output provides the correlation coefficient, the t-statistic, df, p-value, and the 95% confidence interval for the correlation coefficient. The two variables you supply to the function should both be continuous (most likely type `numeric`, `integer`, or `double` in R). 

Note that the t statistic for the correlation depends on the magnitude of the correlation coefficient (r) and the **sample size**. With a large sample, even weak correlations can become statistically significant.

### Relationship with simple linear regression

One might also be led to believe that the correlation coefficient is similar to the slope of a simple linear regression. For one, the test for correlation will more or less lead to a similar conclusion as the test for slope. The sign of the slope (+/-ve) will be the same for correlation, and both values should indicate the direction of the relationship. 

However, those two statistics are different: the correlation coefficient only tells you how closely your data fit on a line, so two datasets with the same correlation coefficient can have very different slopes. In other words, the value of the correlation indicates the _strength_ of the linear relationship. The value of the slope does not. Moreover, the slope interpretation tells you the change in the response for a one-unit increase in the predictor. Correlation does not have this kind of interpretation.


### Pitfalls? 

Outliers and non-linear relationships. 

A simple way to evaluate whether a relationship is reasonably linear is to examine a scatter plot.

P-value evaluates how well your data rejects the **null hypothesis**, which states that there is no relationship between two compared groups.

p-value is the probability of obtaining results as extreme or more extreme, given the null hypothesis is true.

## Effect size



## Statistical power

**Statistical power** is the probability of identifying an interaction effect on a dependent variable with the specified sample characteristics. The most common use of power calculations is to estimate how big a sample you will need.

## Sample variance

Examining in-sample variation and between-sample variation are both helpful when comparing means across two populations. 

The f-statistic is in fact calculated by the between-group variance divided by the within-group variance. 

This is where **ANOVA** (Analysis of Variance) comes into play here, where ANOVA is a statistical test used to analyze the difference between the means of more than two groups.  

## References

- [Correlation and Regression](https://sphweb.bumc.bu.edu/otlt/MPH-Modules/PH717-QuantCore/PH717-Module9-Correlation-Regression/index.html)
- [PennState STAT500](https://online.stat.psu.edu/stat500/lesson/9/9.4/9.4.2)
- [Guide on when to use which statistical tests and when](https://www.scribbr.com/statistics/statistical-tests/)
- [Unpaired t-tests in R](http://www.sthda.com/english/wiki/unpaired-two-samples-t-test-in-r/)
- [ANOVA in R](https://statsandr.com/blog/anova-in-r/)
- [Kruskall-Wallis Test in R](https://statsandr.com/blog/kruskal-wallis-test-nonparametric-version-anova/)
- https://stats.stackexchange.com/questions/1637/if-the-t-test-and-the-anova-for-two-groups-are-equivalent-why-arent-their-assu