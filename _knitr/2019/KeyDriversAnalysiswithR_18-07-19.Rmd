---
title: "Key Drivers Analysis with R"
author: "Martin Chan"
date: "August 5, 2019"
output:                    # DO NOT CHANGE
  prettydoc::html_pretty:  # DO NOT CHANGE
    theme: cayman          # DO NOT CHANGE
    highlight: github      # DO NOT CHANGE
---

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
library(tidyverse)
# prettyjekyll::FormatPost("_knitr/First_Post_13-04-19.Rmd")
```

## Background
This post will provide an introduction on how to perform a **Key Drivers Analysis** (KDA) in R. The term *Key Drivers Analysis* is more commonly used in the market research industry, and refers to a family of methods for measuring the importance of input variables (or key drivers, predictors, independent variables, however you'd like to call them) in predicting the outcome variable. KDA is also known as *relative importance analysis*, which is what the R package [relaimpo](https://cran.r-project.org/web/packages/relaimpo/index.html) is named after. 

In this post, I will provide an overview of the two most common methods to perform a KDA:

- **Lindeman, Merenda and Gold** (abbreviated *LMG*) / **Shapley value regression** - using the package [relaimpo](https://cran.r-project.org/web/packages/relaimpo/index.html)

- **Relative Weights Analysis** - using a package I wrote called [rwa](https://github.com/martinctc/rwa), which is based off the work by [Tonidandel and LeBreton, 2015](https://link.springer.com/article/10.1007/s10869-014-9351-z). In the literature, this is often attributed to Johnson (2000)[^ref1] and sometimes referred to Johnson's Relative Weights Analysis.

There are many variations of KDA, where some techniques go under a different name but are for most intents and purposes the same, e.g. _dominance analysis_[^ref2] is very similar to the LMG approach. Although the majority of KDA methods are based on linear regression, _random forests_ can also be used to calculate variable importance,[^ref3] but this is a subject for another post.

```{r echo=FALSE, out.width = '50%', out.extra='style="float:right; padding:10px"'}
knitr::include_graphics("https://martinctc.github.io/blog/images/street-3230554_1280.jpg")
```

## Example: Growing your hex sticker business
A typical use case in market research is to understand what "drives" **customer satisfaction**, where drivers may include:

- Quality of service
- Speed of service delivery
- Quality of product
- Value for money
- Accessibility / availability
- User experience (if digital)

The question then is to understand: *which of these variables are most important if I wish to maximise customer satisfaction?* KDA can be applied to a diverse range of consumer products and services (and even beyond consumer markets), and in different situations you’re likely to get a different set of potential drivers. In an attempt to make this tutorial more engaging and in my curiosity to see whether this reduces the bounce rate of this post, I shall fabricate a slightly comical business example.

Imagine that you lead an analytics team in a business that manufactures and sells R-themed merchandise (e.g. hex stickers, t-shirts), and the CEO has asked you to provide some analysis on the business’s latest customer survey as input to her upcoming annual strategy planning meeting.[^1] What makes your customers happy, and what factors are the most important? You have some standard grid-type 7-point scale customer satisfaction questions, and you want to show up with something a bit more impressive than some correlation coefficients for your boss on how to grow the hex sticker business.

> On a scale of 1 to 7 where 1 is Extremely Dissatisfied and 7 is Extremely Satisfied, how would you rate your level of satisfaction with [BRAND'S] product quality?

(Example of one of the questions available within the survey)

## Principles behind KDA

As succinctly as possible, what a KDA does is to **infer the relative contribution or importance of several predictor variables to the outcome variable in a regression analysis**. The keyword here is **relative** - importance is a measure of the relationship between a predictor and the outcome variable, *relative* to other predictors in the model. The regression model is based off a select number of predictor variables, and model outputs such as coefficients or $R^2$ (or some transformed version of these values) are then used to represent the importance of the input variables. 

The first step, certainly, then is to have some idea of what you want to be measuring:

1. Select the *outcome variable* that you want the input variables to predict (e.g. customer satisfaction)
2. Select the set of predictor variables in which you want to measure their importance in contributing towards the outcome variable. Like with building any model, it is wise to be discriminate and select relevant and meaningful variables (rather than chuck all the data in).

The next step - or you may prefer to do this before even commencing the exercise - is to decide on a method for running the KDA. One of the main differences between the different KDA methods is how they deal with the issue of **multicollinearity**. The challenge with multiple correlated predictors is that it increases the difficulty in inferring the 'true' relative importance of a given single predictor, as the value of their coefficients will be inflated by other correlated predictors in the model. 

According to Tonidandel and LeBreton (2015),

> multicollinearity makes the partitioning of variance among multiple correlated predictors difficult. Despite these difficulties, researchers continue to rely on commonly available indices (e.g., simple correlation coefficients, partial correlation coefficients, standardized regression coefficients) when partitioning the variance in the criterion that is predicted by multiple (typically correlated) predictors.

For most KDA techniques, the ability to deal with correlated predictors offers an advantage over running a simple correlation matrix when you are trying to understand the relative importance of variables.

The *LMG* / *Shapley* method and Johnson's *Relative Weights Analysis* are two of the most well-known methods for running a KDA, and both have a way to deal with correlated predictors.[^ref4] Here's a brief background to these two methods:

### Background: LMG / Shapley

The *LMG* method as referred to in **relaimpo**, first mentioned in Lindeman, Merenda and Gold (1980), but also Kruskall (1987). The method was later improved by other researchers in the field and became better known as *Shapley value regression* or *dominance analysis* (Budescu, 1993). It may perhaps be more accurate to describe this as a close family of related methods, but I follow the precedent in the existing literature that treats this as a single approach.

> The recommended metric is LMG, which provides a decomposition of the model explained variance into non-negative contributions.

A key feature of this method is that it takes the average of each predictor's (*p*) squared semipartial correlation across all *p*! possible orderings of the predictors, defining predictor importance as the **average contribution to $R^2$ across all possible orderings.

### Background: Relative Weights Analysis

*Relative Weights Analysis* (RWA) is a method that, despite using a completely different approach, obtains very similar results to LMG. Unlike LMG, RWA is essentially a **variable transformation** method (see Johnson and LeBreton), which addresses the multicollinearity problem by first transforming the predictors to _maximally related orthogonal variables_. In other words, these "artificial" variables are highly related to the original set of predictors but are uncorrelated. The regression model is then run with the new orthogonal variables as predictors against the outcome variable.

The main advantage of RWA over the Shapley is its computational speed, especially in the case of a large number of predictors. Whilst the time to compute KDA with Shapley increases *exponentially* with the number of predictors in the model, it only increases by polynomial time for RWA. This method deals with multicollinearity by creating a set of new predictor variables that are maximally related to the original variables, but are uncorrelated (orthogonal) to each other.

According to Tonidandel and LeBreton (2015), Relative Weights Analysis...

> ...decompose[s] the total variance predicted in a regression model (R 2) into weights that accurately reflect the proportional contribution of the various predictor variables.

The output of KDA exercises is typically a relative importance weight (%), which represent the proportion that a factor contributes to the output variable, relative to other factors in the model.

The differences between Shapley and RWA in terms of outputs are...

## Method #1 - KDA with LMG (`relaimpo`)

Let's start with loading in the {relaimpo} package and the example dataset. 

```{r message=FALSE, warning=FALSE}
library(relaimpo)

data <- read_csv(here::here("datasets","kda_example.csv"))
```

This data was collected from our Annual Customer Satisfaction survey, which interviewed a representative sample of 1000 customers who have recently purchased one of our R-themed merchandise. We've asked them to rate on a seven-point scale their satisfaction with a number attributes.

By running `glimpse()`, you'll see that excluding the respondent id, the dataset has five variables:

- `quality_service` - Quality of Service
- `speed_service` - Speed of Service
- `quality_product` - Quality of Product
- `value_for_money` - Value for Money
- `overall_sat` - Overall Satisfaction

```{r}
data %>% glimpse()

# data %>% 
#   filter(overall_sat >= 6) %>%
#   mutate_at(vars(-id, -overall_sat),
#             ~sample(5:7, length(.), replace = TRUE)) %>% copy_df()
```

We can base the KDA off a linear regression model. Let's build one:

```{r}
data %>%
  lm(overall_sat ~ quality_service + speed_service + quality_product + value_for_money,
     data = .) -> lm_fit

lm_fit
```

According to the package documentation, **relaimpo** is also compatible with Thomas Lumley's **survey** package, which allows you to use this package together with complex survey designs.

## Method #2 - KDA with RWA (`rwa`)



## Important Notes
There are two key caveats with KDA:
KDA outputs are relative estimates, and is critically dependent on the inputs selected to build the model. For this analysis we have selected variables deemed most relevant as drivers of eNPS based on the established understanding of the employee market.

Different methods of KDA deliver very similar results on strong relationships, but fluctuate more on weaker relationships. Therefore, consistency across different models should in itself be interpreted as evidence (e.g. benefits consistently ranked higher over income as driver)

#### -------------------------------- ####
# FOOTNOTES!!!
#### -------------------------------- ####

[^1]: KDA is typically applied to survey data, but there can certainly be alternative use cases. See [this tutorial](https://cran.r-project.org/web/packages/dominanceanalysis/vignettes/da-logistic-regression.html) by Filipa Coutinho Soares (2019) which uses dominance analysis (using `dominanceanalysis`) with a binomial logistic regression to identify the most important environmental variables for predicting the presence of tropical birds.

[^ref1]: Johnson, J.W. (2000). "A heuristic method for estimating the relative weight of predictor variables in multiple regression". _Multivariate Behavioral Research_, 35, 1-19.

[^ref2]: Budescu, D. V. (1993). Dominance analysis: a new approach to the problem of relative importance of predictors in multiple regression. _Psychological bulletin_, 114(3), 542.

[^ref3]: Grömping, U. (2009). Variable importance assessment in regression: linear regression versus random forest. _The American Statistician_, 63(4), 308-319.

[^ref4]: These are the two main methods discussed in this excellent overview: Johnson, J. W., & LeBreton, J. M. (2004). History and use of relative importance indices in organizational research. _Organizational research methods_, 7(3), 238-257.
